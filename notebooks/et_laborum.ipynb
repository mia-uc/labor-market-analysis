{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN9modRJeqUD",
        "outputId": "0ef9db9f-1c80-4eec-d986-5a52de18c65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'labor-market-analysis': No such file or directory\n",
            "Cloning into 'labor-market-analysis'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 194 (delta 96), reused 151 (delta 60), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (194/194), 55.41 KiB | 2.22 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "/content/labor-market-analysis\n"
          ]
        }
      ],
      "source": [
        "!rm -r labor-market-analysis\n",
        "!git clone https://github.com/mia-uc/labor-market-analysis.git\n",
        "%cd labor-market-analysis\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "SCRAPER, CLEAN = True, True\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "os.environ['MONGO_CONN_STRING'] = '....'\n",
        "os.environ['MONGO_DB'] = '....'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jRtLBrvEf4ML"
      },
      "outputs": [],
      "source": [
        "from src.scrapers.laborum.scraper import LaborumScraper\n",
        "\n",
        "if SCRAPER:\n",
        "    scraper = LaborumScraper()\n",
        "    scraper.save_all(\n",
        "        n_page=-1,\n",
        "        skips=0,\n",
        "        parallel = False,\n",
        "        # filter_condition = lambda job: job['fechaPublicacion'] == '03-05-2023'\n",
        "    )\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.scrapers.laborum.clean_pipeline import LaborumCleanPipeline\n",
        "from src.etl_process.jobs_db_center import JobsDBCenter\n",
        "\n",
        "if CLEAN:\n",
        "    center = JobsDBCenter()\n",
        "    client_pipeline = LaborumCleanPipeline()\n",
        "    client_pipeline.run(center)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XJ3A8Oen0cr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
